## **Hashed data in a DB collection**

**Purpose** : MongoDB Ops Manager Diagnostic Reports are utilized to extract and transmit metadata for troubleshooting and data analysis related to licensing. To ensure data sensitivity during the analysis of these reports, hashing data within a MongoDB collection is necessary to protect and anonymize sensitive information.

Customer SREs and/or DBAs are requested by MongoDB Support or Product/Sales team members to generate MongoDB Ops Manager Diagnostic Reports to help troubleshoot, monitor, and analyze the health of MongoDB clusters. These reports capture performance metrics, logs, and system information to assist in diagnosing issues.


**What Do Ops Manager Diag Reports Contain?**

Diag reports include several key components:

1. Cluster & Node Information
- Replica Set / Shard details
- Primary & Secondary status
- Election history & failover events

2. Performance Metrics
- CPU, Memory, and Disk Usage
- Query Execution Statistics
- Locking & Contention Details

3. Logs & Errors
- MongoDB Logs (mongod, mongos, opsmgr)
- Slow Query Logs & Profiler Data
- Replication Lag & Oplog Health

4. Storage & Indexing
- Index Usage & Fragmentation
- WiredTiger Cache Metrics
- Disk Read/Write Throughput


**What happens after Ops Manager Diag Reports are generated by customers?**
Ops Manager Diag report(s) are then uploaded to the support ticket which is then accessed by MongoDB Support team or the SA team. 

For licensing analysis purposes, the SA :

1. Downloads these Diag reports from the Support Ticket.
2. Unzip the ‘diagnostics_<<date>>.tar.gz’ on your local machine.
3. Locate ‘hosts.json’ file in the extracted folder.
4. Import the ‘hosts.json’ file in an empty DB collection. For eg. In MongoDB Compass, use the “Add Data” > “Import JSON or csv file” option to upload hosts.json file in an empty DB collection.
5. Review the imported data, noting that sensitive information (such as hostname, IP address, and file paths, etc) appears in plain text. This can pose a potential security risk, which some customers may want to mitigate.
6. This is where the attached scripts help with data analysis and also data hashing.


**Analysis of uploaded hosts.json file**

**hosts_agg.txt** : sample aggregation pipelines can be executed on the DB Collection to :
1. Match the records with ‘slt’ > a given date and other conditions as called upon in the “match stage”. 
2. Group by fields like rsid nodes, version, .., .., ..
3. Project the relevant fields required from the collection.

**coll_hash.py** : This is python script that :
1. Takes input from you for name of db collection and the list of fields you want to hash in that collection.
2. Creates new hashed fields in the same collection.
3. Hashed and Clear text fields exist in same collection. Once done, you can either, create a view with read access granted on only this view, or grant select on hashed fields in the collection via RBAC to all users.


**A couple comments while using coll_hash.py script**
1. Coded (masked) data cannot be decoded. i.e. if a malicious actor gets hold of masked data string they won't be able to decode it to it's original text.
2. Data Masking will always be the same. i.e. “MongoDB” will always be masked/redacted to a string (for eg.) “FlexibleDocumentDB”.
3. Users can now run queries on the collection/view and then query, join masked or clear text data as per their choice, and potentially create report only based on masked data.
